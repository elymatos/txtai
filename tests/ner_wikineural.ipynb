{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b67102-8cfa-4c69-a3c2-1f0aba19189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes==0.43.1\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q transformers[sentencepiece]\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd14ecc-921b-4cdb-a2f1-51792faa16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 14:57:44.973801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f39a9e7b3154f0289c221a07a59f42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909e883fbd094eb9969633bbb1006585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36171cc28bf6441084bf88155880bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d15016358204d5b8169d518b5fa9f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ematos/miniconda3/envs/gpu-env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1614: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff313ebe7434dcfa27462c7a2b54a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8403e892c54b42888c4089aa4f16650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/709M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e987c0e5-e460-410d-b6a5-fa5c23a2df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/home/ematos/miniconda3/envs/gpu-env/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6d4895-74a5-441a-9e90-2ee521cad3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../files/texto01.txt'\n",
    "text = open(filepath, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d96d6a-1acf-47d7-8b75-2851b605a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.5869531, 'word': 'Senhora Senhora Rivail', 'start': 4, 'end': 27}, {'entity_group': 'PER', 'score': 0.81715226, 'word': 'Boudet', 'start': 57, 'end': 63}, {'entity_group': 'LOC', 'score': 0.99497426, 'word': 'Château - du - Loir', 'start': 75, 'end': 90}, {'entity_group': 'LOC', 'score': 0.6078252, 'word': 'Sarthe', 'start': 92, 'end': 98}, {'entity_group': 'PER', 'score': 0.9522022, 'word': 'Amélie', 'start': 286, 'end': 292}, {'entity_group': 'PER', 'score': 0.87714845, 'word': 'Crispoul', 'start': 792, 'end': 800}, {'entity_group': 'PER', 'score': 0.87948895, 'word': 'Grandins', 'start': 896, 'end': 904}, {'entity_group': 'PER', 'score': 0.93088436, 'word': 'Leroux', 'start': 909, 'end': 915}, {'entity_group': 'PER', 'score': 0.9403838, 'word': 'Ferran', 'start': 1360, 'end': 1366}, {'entity_group': 'PER', 'score': 0.42519313, 'word': 'Ballet', 'start': 1461, 'end': 1467}, {'entity_group': 'PER', 'score': 0.95294714, 'word': 'Morin', 'start': 1829, 'end': 1834}]\n"
     ]
    }
   ],
   "source": [
    "ner_results = nlp(text)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5010b0a5-7ee8-4d41-9904-288ffe8af710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filepath = '../files/texto01_result.txt'\n",
    "f = open(filepath, \"w\")\n",
    "f.write(json.dumps(str(ner_results)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af4dc737-1e44-49fc-a48f-fa847b4e227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector as mariadb\n",
    "import sys\n",
    "\n",
    "# Connect to MariaDB Platform\n",
    "try:\n",
    "    conn = mariadb.connect(\n",
    "        user=\"kardec\",\n",
    "        password=\"\",\n",
    "        host=\"projetokardec.ufjf.br\",\n",
    "        port=33069,\n",
    "        database=\"kardec_db\"\n",
    "\n",
    "    )\n",
    "except mariadb.Error as e:\n",
    "    print(f\"Error connecting to MariaDB Platform: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get Cursor\n",
    "cur = conn.cursor()\n",
    "# Select tagged sentences without a NLP label. NLP labels are set for non-informative sentences.\n",
    "cur.execute(\"SELECT idSentence as id, text from ak_sentence where idlanguage=1\")\n",
    "#cur.execute(\"SELECT idSentence as id, text from ak_sentence where idlanguage=1 and (idSentence < 50)\")\n",
    "rows = cur.fetchall()\n",
    "filepath = '../files/full_result.txt'\n",
    "f = open(filepath, \"w\")\n",
    "for row in rows:\n",
    "#    print(row[0], row[1])\n",
    "    ner_results = nlp(row[1])\n",
    "    for result in ner_results:\n",
    "        if (result['entity_group'] == 'PER') or (result['entity_group'] == 'LOC'):\n",
    "            f.write(str(row[0]) + ',' + result['entity_group'] +  ',' + result['word']+ \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6f58a-9e00-45ac-b849-7e23c367171a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
